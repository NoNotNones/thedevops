[
{
	"uri": "http://localhost:1313/thedevops/",
	"title": " DevOps Projects ",
	"tags": [],
	"description": "",
	"content": "Projects DevOps is a methodology that combines software development and IT operations to create a streamlined and automated development process. To achieve this, there are several tools that DevOps teams use to automate processes, manage infrastructure, and monitor applications\nIn this workshop, we will focus in some of DevOps tools:\nDevOps tools categorized: Version Control Systems (VCS): GitHub, GitLab Continuous Integration (CI) Tools: Jenkins, GitLab Containerization and Orchestration: Docker, Kubernetes Infrastructure as Code (IaC): Terraform Monitoring and Logging Tools: Prometheus, Zabbix, Grafana, ELK Stack Content Introduction Prepairation Configuration Cleanup "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.1-jenkins/6.1.1-cicd/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "In a Jenkins CI/CD setup, an agent is a worker node that performs the tasks defined in the Jenkins pipeline.\nAgent Hostname: Lab-Server\nRequired: Installed JAVA Configuration:\nInstall JAVA: apt install openjdk-11-jdk -y Create User: adduser jenkins Create working folder: mkdir /var/lib/jenkins Permission: chown jenkins. /var/lib/jenkins cd /var/lib/jenkins/ su jenkins Jenkins-Server Add Node: Dashboard \u0026gt; Manage Jenkins \u0026gt; Nodes \u0026gt; New node\nNode name: lab-server Number of excute: 1 Lables: lab-server Custom workdir path: /var/lib/jenkins Add inbound port: 8999 Manage Jenkins - Security\nAgent - Fixed: 8999 Connect Jenkins to Gitlab-Server "
},
{
	"uri": "http://localhost:1313/thedevops/2-prepair/2.1-docker/",
	"title": "Container",
	"tags": [],
	"description": "",
	"content": "A container is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries, and settings\nOverview Docker Docker is a platform that enables developers to build, package, ship, and run applications in containers. It provides tools and a platform to manage containerized applications across different environments, from development to production. Docker Compose Docker Compose is a tool provided by Docker that allows you to define and manage multi-container Docker applications. It uses a YAML file to configure the services, networks, and volumes required for your application Configuration Check the installed software\ndocker --version docker-compose --version Create a docker compose file to run the software on the container environment\ndocker-compose.yml version: \u0026#39;3\u0026#39;\rservices:\r# Terraform\rterraform:\rimage: hashicorp/terraform:latest\rvolumes:\r- .:/terraform\rworking_dir: /terraform\r# AWS CLI\u0026#39;\raws:\rimage: anigeo/awscli\renvironment:\rAWS_ACCESS_KEY_ID: \u0026#34;${AWS_ACCESS_KEY_ID}\u0026#34;\rAWS_SECRET_ACCESS_KEY: \u0026#34;${AWS_SECRET_ACCESS_KEY}\u0026#34;\rAWS_REGION: \u0026#34;${AWS_REGION}\u0026#34;\rAWS_DEFAULT_REGION: ap-southeast-1\rvolumes:\r- $PWD:/app\rworking_dir: /app "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.4-security/6.4.1-firewall/",
	"title": "Firewall",
	"tags": [],
	"description": "",
	"content": "Firewall security is processing \u0026hellip;.\npfSense: ntop-ng pfSense is an open-source firewall and router software distribution based on FreeBSD. It offers a wide range of features, including firewall functionality, routing, VPN support, and more. One of the powerful add-ons available for pfSense is ntopng (NTOP Next Generation), which provides deep packet inspection and traffic analysis capabilities. ntop-ng will monitoring network traffic and identifying potential security threats or policy violations, ntopng helps enhance network security and ensures compliance with regulatory requirements. It can detect suspicious activities, malware infections, unauthorized access attempts, and other security incidents in real-time. Overview Configuration Dashboard "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.5-linux/6.5.1-frontend/",
	"title": "Frontend",
	"tags": [],
	"description": "",
	"content": "Overview Vagrant and Oracle VM Vitualbox Manager\nConfiguration Server name: machine1 - IP: 192.168.6.100\nPS D:\\LAB\\Vagrant\u0026gt; vagrant up machine1\rvagrant ssh machine1 Copy source to machine: scp -i d:\\LAB\\Vagrant\\.vagrant\\machines\\machine1\\virtualbox\\private_key \\LAB\\Vagrant\\datas\\todolist.zip vagrant@192.168.6.100:/home/vagrant Setup: sudo -i\rapt update\rapt install unzip\runzip todolist.zip\rmkdir /projects\rmv todolist /projects\rls -l /projects Add user for project: adduser todolist\rchown -R todolist:todolist /projects/todolist\rchmod -R 750 /projects/todolist Run Project: VUE Source :\nhttps://vuejs.org/guide/quick-start Prerequisites: Node.js - https://www.digitalocean.com/community/tutorials/how-to-install-node-js-on-ubuntu-20-04 Install Aplication\napt update\rapt install nodejs\rapt install npm Configure foler and user\nsudo todolist\rcd /projects/todolist Review configure file:\npackage.json {\r\u0026#34;name\u0026#34;: \u0026#34;todolist\u0026#34;,\r\u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;,\r\u0026#34;private\u0026#34;: true,\r\u0026#34;scripts\u0026#34;: {\r\u0026#34;serve\u0026#34;: \u0026#34;vue-cli-service serve\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;vue-cli-service build\u0026#34;\r},\r\u0026#34;dependencies\u0026#34;: {\r\u0026#34;bootstrap-vue\u0026#34;: \u0026#34;^2.22.0\u0026#34;,\r\u0026#34;core-js\u0026#34;: \u0026#34;^3.8.3\u0026#34;,\r\u0026#34;vue\u0026#34;: \u0026#34;^2.6.14\u0026#34;,\r\u0026#34;vue-router\u0026#34;: \u0026#34;^3.5.1\u0026#34;\r},\r\u0026#34;devDependencies\u0026#34;: {\r\u0026#34;@vue/cli-plugin-babel\u0026#34;: \u0026#34;~5.0.0\u0026#34;,\r\u0026#34;@vue/cli-plugin-router\u0026#34;: \u0026#34;~5.0.0\u0026#34;,\r\u0026#34;@vue/cli-service\u0026#34;: \u0026#34;~5.0.0\u0026#34;,\r\u0026#34;vue-template-compiler\u0026#34;: \u0026#34;^2.6.14\u0026#34;\r},\r\u0026#34;browserslist\u0026#34;: [\r\u0026#34;\u0026gt; 1%\u0026#34;,\r\u0026#34;last 2 versions\u0026#34;,\r\u0026#34;not dead\u0026#34;\r]\r} vue.config.js const { defineConfig } = require(\u0026#39;@vue/cli-service\u0026#39;)\rmodule.exports = defineConfig({\rtranspileDependencies: true\r})\rmodule.exports = {\rdevServer: {\rport: 3000\r},\r}; Run project\nnpm install\rnpm run build ISSUE: version Fix ISSUE: Update same version or latest\ncurl -s https://deb.nodesource.com/setup_18.x | sudo bash\rsudo apt install nodejs -y\rnode -v\rnpm run build Project will build file or folder. In this project VUE will build to a folder is : dist\nWe have 3 way to run a Fontend project: Webserver - Services - Pm2\nnpm run serve Checking : http://192.168.6.100:3000/\nRun with Nginx Webserver Install\napt install nginx -y Check port\nnetstat -tlpun Configure:\nChange default nginx port to 8999: cd /etc/nginx\nvi sites-avaiable/default server {\rlisten 8999 default_server;\rlisten [::]:8999 default_server; Test and apply configure: nginx -t and systemctl restart nginx Setup Project run Nginx port 8088:\nCreate configure file: vi conf.d/todolist.conf server {\rlisten 8088;\rroot /projects/todolist/dist/;\rindex index.html;\rtry_files $uri $uri/ /index.html;\r} ISSUE: User for nginx (vi /etc/nginx.conf -\u0026gt; user www-data) Fix: add user (www-data) to group todolist\nusermod -aG todolist www-data Apply configure: systemctl restart nginx or nginx -s reload Projects React Source: vision.zip\nUser: vision\nProject folder: /projects/vision\nRun application\nnpm install Run as services: vi /lib/systemd/system/vision.service\n[Service]\rType=simple\rUser=vision\rRestart=on-failure\rWorkingDirectory=/projects/vision/\rExecStart=npm run start -- --port=3000 systemctl daemon-reload systemctl start vision systemctl status vision Check: http://192.168.6.100:3000\n"
},
{
	"uri": "http://localhost:1313/thedevops/3-config/3.1-ec2/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "In this Workshop we will create an EC2 instances with the information bellow\nOverview AWS EC2 Instances name: Web-Server VPC: 10.0.0.0/16 Subnets: 10.0.1.0/24 Region: Singapore (ap-southeast-1) Available zone: ap-southeast-1b Instance type: t2.micro Amazon Machine Images: Amazon Linux 2 AMI Key pair: tf-cli-keypair Security setting: Only allow my ip connect SSH to EC2 instance Allow all access from port 8080 to EC2 instance Terraform configuration Security credential variables: variables.tf\nvariable \u0026#34;access_key\u0026#34; {\rtype = string\rsensitive = true\r}\rvariable \u0026#34;secret_key\u0026#34; {\rtype = string\rsensitive = true\r}\rvariable \u0026#34;region\u0026#34; {\rtype = string\rdefault = \u0026#34;ap-southeast-1\u0026#34;\r} Instances configurations :main.tf\nvariable vpc_cidr_block {}\rvariable subnet_1_cidr_block {}\rvariable avail_zone {}\rvariable env_prefix {}\rvariable instance_type {}\rvariable my_ip {}\rvariable ami_id {}\rresource \u0026#34;aws_vpc\u0026#34; \u0026#34;myapp-vpc\u0026#34; {\rcidr_block = var.vpc_cidr_block\rtags = {\rName = \u0026#34;${var.env_prefix}-vpc\u0026#34;\r}\r}\rresource \u0026#34;aws_subnet\u0026#34; \u0026#34;myapp-subnet-1\u0026#34; {\rvpc_id = aws_vpc.myapp-vpc.id\rcidr_block = var.subnet_1_cidr_block\ravailability_zone = var.avail_zone\rtags = {\rName = \u0026#34;${var.env_prefix}-subnet-1\u0026#34;\r}\r}\rresource \u0026#34;aws_security_group\u0026#34; \u0026#34;myapp-sg\u0026#34; {\rname = \u0026#34;myapp-sg\u0026#34;\rvpc_id = aws_vpc.myapp-vpc.id\ringress {\rfrom_port = 22\rto_port = 22\rprotocol = \u0026#34;tcp\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\r}\ringress {\rfrom_port = 8080\rto_port = 8080\rprotocol = \u0026#34;tcp\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\r}\regress {\rfrom_port = 0\rto_port = 0\rprotocol = \u0026#34;-1\u0026#34;\rcidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;]\rprefix_list_ids = []\r}\rtags = {\rName = \u0026#34;${var.env_prefix}-sg\u0026#34;\r}\r}\rresource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;myapp-igw\u0026#34; {\rvpc_id = aws_vpc.myapp-vpc.id\rtags = {\rName = \u0026#34;${var.env_prefix}-internet-gateway\u0026#34;\r}\r}\rresource \u0026#34;aws_route_table\u0026#34; \u0026#34;myapp-route-table\u0026#34; {\rvpc_id = aws_vpc.myapp-vpc.id\rroute {\rcidr_block = \u0026#34;0.0.0.0/0\u0026#34;\rgateway_id = aws_internet_gateway.myapp-igw.id\r}\r# default route, mapping VPC CIDR block to \u0026#34;local\u0026#34;, created implicitly and cannot be specified.\rtags = {\rName = \u0026#34;${var.env_prefix}-route-table\u0026#34;\r}\r}\r# Associate subnet with Route Table\rresource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;a-rtb-subnet\u0026#34; {\rsubnet_id = aws_subnet.myapp-subnet-1.id\rroute_table_id = aws_route_table.myapp-route-table.id\r}\routput \u0026#34;server-ip\u0026#34; {\rvalue = aws_instance.myapp-server.public_ip\r}\rresource \u0026#34;aws_instance\u0026#34; \u0026#34;myapp-server\u0026#34; {\rami = var.ami_id\rinstance_type = var.instance_type\rkey_name = \u0026#34;tf-cli-keypair\u0026#34;\rassociate_public_ip_address = true\rsubnet_id = aws_subnet.myapp-subnet-1.id\rvpc_security_group_ids = [aws_security_group.myapp-sg.id]\ravailability_zone\t= var.avail_zone\rtags = {\rName = \u0026#34;${var.env_prefix}-server\u0026#34;\r}\r} Terraform provider AWS : terraform.tfvars\n# Network and Instance variables\rvpc_cidr_block = \u0026#34;10.0.0.0/16\u0026#34;\rsubnet_1_cidr_block = \u0026#34;10.0.1.0/24\u0026#34;\ravail_zone = \u0026#34;ap-southeast-1b\u0026#34;\renv_prefix = \u0026#34;web\u0026#34;\rmy_ip = \u0026#34;\u0026lt;myip\u0026gt;/32\u0026#34;\rami_id = \u0026#34;ami-04f73ca9a4310089f\u0026#34; Installation Terraform plan:\ndocker-compose run –rm terraform plan Terraform apply:\ndocker-compose run --rm terraform apply --auto-approve AWS Instance checking: Add Keypair permission:\nchmod 400 tf-cli-keypair.pem SSH to EC2 Instances:\nssh -i tf-cli-keypair.pem ec2-user@13.250.64.49 AWS Instance checking: "
},
{
	"uri": "http://localhost:1313/thedevops/1-intro/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "DevOps Projects Overview Continuous Integration and Continuous Deployment (CI/CD) is a cornerstone of DevOps methodology, emphasizing automation and collaboration throughout the software development lifecycle.\nContinuous Integration (CI): Developers regularly push code changes to a shared repository. Automated tests are run against the newly integrated code to ensure it doesn\u0026rsquo;t break existing functionality. If tests fail, developers are notified immediately, and they can address issues before they become larger problems. Continuous Deployment (CD): Once code changes pass the CI phase, they\u0026rsquo;re automatically deployed to production or staging environments. This process reduces manual intervention and ensures that new features or bug fixes reach users as quickly as possible. Content Introduction Prepairation Configure Cleanup "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.1-jenkins/",
	"title": "Jenkins",
	"tags": [],
	"description": "",
	"content": "Overview Jenkins is an open-source automation server that is widely used for automating software development processes such as building, testing, and deploying applications. It allows developers to automate repetitive tasks associated with the software development lifecycle, thereby saving time and reducing errors.\nLogin vagrant ssh machine2 Install Jenkins server in Linux Login Jenkins-Server Installation : Step by step\r# Change to the root user after logging in: sudo su - # Add the aptitude key for the Jenkins application: wget -q -O - https://pkg.jenkins.io/debian-stable/jenkins.io.key | sudo apt-key add - # Add the Jenkins debian repo to the aptitude sources list: echo \u0026#34;deb https://pkg.jenkins.io/debian-stable binary/\u0026#34; \u0026gt; /etc/apt/sources.list.d/jenkins.list # Update the source lists and upgrade any out of date packages: apt update apt -y upgrade # Install the software for the Jenkins master: openjdk-11-jdk, nginx, and jenkins. # Install JDK and nginx first: apt -y install openjdk-11-jdk nginx # Then install jenkins: apt -y install jenkins # Confirm that jenkins and nginx are installed: systemctl status nginx | grep Active systemctl status jenkins | grep Active Install via shell scripts (checking ...)\rvi **jenkins-install.sh**\r#!/bin/bash apt install openjdk-11-jdk -y java --version wget -p -O - https://pkg.jenkins.io/debian/jenkins.io.key | apt-key add - sh -c \u0026#39;echo deb http://pkg.jenkins.io/debian-stable binary/ \u0026gt; /etc/apt/sources.list.d/jenkins.list\u0026#39; apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 5BA31D57EF5975CA apt-get update apt install jenkins -y systemctl start jenkins systemctl enable jenkins ufw allow 8080 chmod +x jenkins-install.sh\rsh jenkins-install.sh\rsystemctl status jenkins\rInstall Jenkins server running in docker container Step by step\rPrepair sudo -i\rapt update\rmkdir /tools\rcd /tools\rmkdir docker\rcd docker\rvi install-docker.sh File content: install-docker.sh\n#!/bin/bash sudo apt update sudo apt install -y apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt update sudo apt install -y docker-ce sudo systemctl start docker sudo systemctl enable docker sudo curl -L \u0026#34;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker --version docker-compose --version Install chmod +x install-docker.sh\rsh install-docker.sh Create Docker compose file mkdir /tools/jenkins\rcd /tools/jenkins\rvi docker-compose.yml version: \u0026#39;3\u0026#39; services: # Jenkins jenkins: container_name: jenkins image: jenkins/jenkins ports: - \u0026#34;8080:8080\u0026#34; volumes: - jenkins_home:/var/jenkins_home volumes: jenkins_home: Run Jenkins Server docker-compose up jenkins -d Configuration Jenkins Server: Login: http://192.168.33.110:8080\nConfiguration\nAdmin Configuration: expand\r## Admin password: docker exec -it jenkins cat /var/jenkins_home/secrets/initialAdminPassword Dashboard Overview Create a pipeline project Content Jenkins CICD Advanced "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.1-sre/",
	"title": "SRE",
	"tags": [],
	"description": "",
	"content": "DevOps Foundations: Site Reliability Engineering Your job as a Devops SRE teams are half software engineers and half systems engineers. The real key is the engineering. It\u0026rsquo;s an automation first approach that has little tolerance for repeated manual work.\nContinuous delivery is a DevOps approach to build engineering, Infrastructure automation is a DevOps approach to systems infrastructure SRE is a DevOps approach to the production support part of operations SRE is “what happens when you ask a software engineer to design an operations function.” —Ben Traynor ITSM \u0026amp; SRE\nITSM tried to solve these problems with paperwork SRE tries to solve them with engineering One way to make an assessment of where your organization currently is on its DevOps journey is to use a maturity mode: Operations-Maturity-Model Linkedin\nWe like the operations maturity model, and we\u0026rsquo;ve included a copy of it in the course handout\nTo give an example, let\u0026rsquo;s look at one part of the model: Revisiting it six months later gives a good scorecard for the future. And by understanding the eventual goal, you can organically improve your culture, tools, and processes.\nSRE Practice Areas : 1. Release engineering is mainly about coming up with the best, lowest friction process you can to verify and to prove changes, and to communicate their release to production that works for you and your business. That\u0026rsquo;s right, continuous delivery shouldn\u0026rsquo;t mean continuous disruption. People need to know what\u0026rsquo;s changing and when - One great tool to do this is Feature Flags. Basically, as you make changes, you wrap them in toggles. 2. Change management : The goal is to have your change and release process eliminate manual work and instead rely on automation. The Visible Ops Handbook Phase 1 – stabilize the patient, modify first res Phase 2 – catch and release, find fragile artifac Phase 3 – establish repeatable-build library Phase 4 – enable continuous improvement 3. Seft-service automation: When following an SRE approach, all of this falls into the topic of self-service automation. Self-service automation is much like the title implies.Individuals inside the organization are able to do what they need to through automation. Instead of creating silos between teams, we create automation to deal with the tasks most often needed where teams interact with each other. There are two main benefits of creating self-service automation One of the benefits behind self-service automation is that it lowers the work in progress. The second benefit is reliability 4. SLAs and SLOs Service Level Agreements and Service Level Objectives. Service Level Agreements are the promises you make about your service to others, whether your customers, whether they\u0026rsquo;re people or other services. Think of SLAs as the services contract. Service level objectives are the goals that your service strives to provide in order to be deemed successful. Think of SLOs as the goals for the service. Knowing what metrics to use for SLAs, and SLOs makes all the difference down the road When choosing these service metrics, there are a few considerations: Frequency: How often is the measurement taken of your symmetric? Every 10 seconds? Every minute? Aggregating: over a certain time period, and averaging the measurements. Depending on the service, this can be wildly different. Geography: Are there different goals for certain regions or geography? For example, ensuring the same service performance in different areas of the globe has widely varying costs, and architectural concerns associated with it. Monitoring system being used to measure the metric is key. 5. Incident management: Problems with production are not usually people\u0026rsquo;s favorite part of the job. But problems do strike, and having a high quality playbook on how to handle them will reduce your downtime.\nThere are tools that address these issues. There are also open source alternatives like Cabot and Open Duty. - These tools let you set up on-call rotations and alert routing, and engineers can set up their own contact methods. Then when an alert comes in, the tools can reach out via email, text, and even voice calls to get someone to respond.\nCommunicating the status of a problem to users is almost as important as getting someone to even fix it. Users depend on your service, and when there\u0026rsquo;s an issue, they expect to be proactively notified with clear information on how it impacts them, and when to expect a resolution. - You should have a robust incident communication process that is linked with your incident response process\nThe more you have clear, easy-to-follow incident management processes, and engineers can escalate to get the help they need, the less stressful on-call is. 6. Introducting postmortems Once services get restored after an incident, your job is only half over. - That\u0026rsquo;s right, being a good SRE requires learning from failures and outages. In a blameless postmortem, engineers share what happened and what they did, even the mistakes, without fear of punishment or retribution. How do we trun postmortems: In a complex system there is not a single reason that a failure happened. All engineering systems are complex and you already have various safeguards in place, tests, monitoring, and so on For an outage to occur, it usually requires many failures at a variety of levels. 7. The postmortem process In our course handout, we have provided a postmortem template we like to use. Postmortem Template.pdf\nLet\u0026rsquo;s break down how we should use them.\nFirst is the description: The description should be short, and capture the essence of the incident in just a couple of sentences. The timeline includes relevant events, including activities that happened before, during, and after the incident. Next, you will want to link in any sources: This could be customer tickets, monitoring graphs, log line snippets, or anything else relevant to documenting the outage. Then we have a section on what went well: It\u0026rsquo;s easy to jump to new theories of how to make your systems more perfect, but it\u0026rsquo;s also valuable to look at what safeguards are already working to help reduce frequency, impact, or length of service issues. The next section is contributing causes: one bullet point per thing that contributed to the failure happening, delayed its detection, or delayed its resolution. - Include issues with communication or process as well Lastly, we round this out with corrective actions: . What specific things can we do in the future to improve our code, systems, and our response to outages? Finally, we calculate the metrics: escribe when the beginning and to end of the incident occurred, as well as calculate time to detection and time to resolution. Next, we gather this to stakeholders and hold the postmortem meeting within a few days of the incident Postmortem Template sample: Title :\n[Write three-to-five-word description of the incident.] Incident description:\n[Write two or three sentences describing the incident. Make it understandable to management and business stakeholders.] How was the incident detected? [Describe how the incident was detected. Include who, what, where, why, and how.] How were customers affected? [Describe which customers were impacted and the exact impact. Include who, what, where, why, and how. If customers reported issues, link those in here as well.] Timeline\n[Everything should be in the same time zone.]\nPre-incident • 01/01/2019 00:00:00 UTC [Add relevant information in chronological order.] Incident • 01/01/2019 00:00:00 UTC [Add relevant information in chronological order.] Post-incident • 01/01/2019 00:00:00 UTC [Add relevant information in chronological order.] Sources • [Links to monitoring, dashboards, logs] What went well\n[Identify things that worked well to fend off for help resolve the incident. Each should be brief.] Contributing causes\n[Identify factors that caused the incident, delayed its detection, or delayed its resolution. Each should be brief.] Corrective actions\n[Describe what changes have or will be taken to prevent or mitigate this or similar incidents in the future. Changes listed here include but are not limited to testing, alerting, monitoring,logging, backups, and anything else related. Doing postmortems is an evolving practice inside the organization, so changes to the postmortem process itself should be included as well. Add relevant links to these items tracked in ticketing systems.] Metrics\nWhen did the incident begin? [first errors recorded] When did the incident end? [service restored] When did we detect the incident? [alert fired/customer notified] Time to detect [start_time – detect_time] Time to resolve [start_time – end_time] 8. Troubeshooting It\u0026rsquo;s an inherent part of fixing service issues\nTroubleshooting is essentially a form of inquiry.\nYou ask a question, devise a hypothesis, construct an experiment, test, analyze data, draw a conclusion and iterate. Your first step is to define your problem Often you have a very preliminary, incomplete and possibly erroneous error report, maybe an alert or someone who insists the server is down. Record the exact behavior you\u0026rsquo;re seeing and start asking questions What\u0026rsquo;s the scope of the problem? Is the problem with that entire network? All the services in that cluster? All the processes on that one server? Is it just that endpoint? Just that user? Is it intermittent? And if so, is there a pattern to it? Has there been a change released to that service or system recently? Is there unusual traffic or are there errors on that application? For each question, determine an experiment you can perform to prove or disprove it. For example, if you have a web transaction that goes through six different tiers, don\u0026rsquo;t jump around. Gather your data to validate each hypothesis\nBring up preexisting data like monitoring and logging data and change records. Then collate the results of the experiments you performed.\nMake sure every experiment can rule out a specific hypothesis or else it\u0026rsquo;s a waste of time.\nDon\u0026rsquo;t waste time debating when there\u0026rsquo;s a production outage at hand. If there\u0026rsquo;s a safe and easy test to rule it out, do that.\nBy logically experimenting, you can quickly get to the root of the problem.\nFirst, try experiments that reduce the scope of the problem the most. For example, if you have a web transaction that goes through six different tiers, don\u0026rsquo;t jump around.\nAnchor on one instrumented point and then work quickly and steadily from it so that you don\u0026rsquo;t end up with information that doesn\u0026rsquo;t make sense and it\u0026rsquo;s hard to correlate Also, prioritize non-intrusive tests over experiments that could affect the system. You\u0026rsquo;ll have to judge whether the risk of those experiments is worse than the problem at hand. And if you can perform those experiments in a test environment, do so. If you have a sufficiently production-like test environment, it\u0026rsquo;s informative to know whether the issue exists there And even if it doesn\u0026rsquo;t, you can reduce the impact of tests you\u0026rsquo;re going to run on production by making sure they work right on the test environment and don\u0026rsquo;t break anything themselves. Of course you should know your own system and the experiments and the tools you use should be documented or better, automated to speed up the process The more parts of your system you can instrument, the easier it is to determine the exact problem. Don\u0026rsquo;t get caught up too much in finding root cause. Remember that the first job of an operator is to restore service to users. Root cause can wait. Once you\u0026rsquo;ve characterized a problem enough to determine a possible resolution, it\u0026rsquo;s worth taking steps to save state Copy logs, take thread dumps, save copy of that VM to look at later, but then get to restoring the service by doing whatever is needed. Fixes follow the same cycle of hypothesis, test and analyze. But be careful, most problems aren\u0026rsquo;t constant over time. It\u0026rsquo;s easy to make a change that you think fixed the problem But for example, what really happened was a buggy script happened to finish executing right around the same time.\nToo often, we allow correlation to control our thinking. Scientific experimentation can\u0026rsquo;t be blind. It has to be based in knowledge of your system and an understanding of why a change would have a given effect Here\u0026rsquo;s a couple of pro troubleshooting tips.\nFirst, you need to know exactly what the system looks like when it\u0026rsquo;s working I've seen people waste hours running down some log error that turned out to be a normal part of that application Also in a distributed application, there are often many external dependencies, like databases, file systems, caches, other services. Knowing them and being able to quickly narrow down which are okay and which are having problems, that\u0026rsquo;s one of the most time saving techniques you can have prepared. And finally, it\u0026rsquo;s unusual for a service disruption to be from just one cause. Often, it takes a couple factors working in concert to disrupt service You know, keep that in mind because multiple variables can cause more complex and confusing symptoms. It takes even more careful experiments to discern the actual problem. Remember, troubleshooting is a science. Don\u0026rsquo;t waste time shooting from the hip. Carefully follow a rational methodology to quickly and reliably lead you to the problem and its fix. 9. Performace Engineering Performance is one of the greatest challenges in operating a reliable system. Once you get the basics down, usually services aren\u0026rsquo;t just all up or all down. Instead, they get slow. Pieces go down and then other parts get overwhelmed and so on. You need to start with a complete view of the problem. This is a reminder to always be monitoring and looking to improve the whole performance of the system Don\u0026rsquo;t spend too much time on the parts that aren\u0026rsquo;t most of the problem The lean adage, see the whole, is especially important to performance. You need to understand what you\u0026rsquo;re optimizing and why. - Tie this to business goals or dollars as much as possible. Start with the most important things and the largest source of delay in those things, then you can apply all your hard-won technical know-how to that to get the most bang for your buck. Good monitoring instrumentation is important to help you distinguish how your systems, network and applications contribute to your overall performance. Transaction tracing is one important tool. Once found only in expensive application performance management tools, there are now open-source tools like OpenTrace and Zipkin that can trace the entire path of a given request through your system. Performance engineering is where you need the most in-depth highest-resolution data. 10. Capacity and scalability There comes a time when you need to serve more users, more hits and more data, and you can\u0026rsquo;t just make due with less. As you grow, you\u0026rsquo;re faced with a challenge of scaling your systems to keep up. - And that\u0026rsquo;s a more multidimensional problem Everything\u0026rsquo;s available in the cloud. I haven\u0026rsquo;t used physical infrastructure since 2014. While a given cloud server is slightly more expensive than running the hardware yourself, the ability to perform short-term scaling, and the better controls you have around configuration billing and so on make for a much better capacity, and scalability story 11. Distributed design Designing applications is a big topic that\u0026rsquo;s near and dear to our hearts. - To do it right, you first need to understand that there\u0026rsquo;s a difference between science and engineering. These are 12 factors that make for a successful distributed application, including how to manage external dependencies, how to store state, and how to scale. Here\u0026rsquo;s one example. Take a service that starts to experience timeouts from one of its backends The requests keep coming in, and the service has more and more slow pending requests until it finally gets overloaded and it crashes. Or even worse, you detect that slow performance and deliberately scale up your front end tier as a result, which puts even more load on the backend, virtually guaranteeing that it\u0026rsquo;ll be overloaded and crash even more quickly This deliberately returns errors quickly if it\u0026rsquo;s backend goes over a certain error level to avoid failure. That\u0026rsquo;s right. Observability means that your service is clearly presenting what\u0026rsquo;s going on with it. In other words, it\u0026rsquo;s logging useful things, and emitting metrics, and maybe has a status endpoint that expresses what\u0026rsquo;s going on with it. Exactly. Then controllability is your ability to control the state of your service. - Being able to pause and resume your apps processing, or turn features on and off with feature flags at runtime 12. Deliberate adversity If it hurts, do it more often. - You sound like that annoying guy at the gym. Can we just skip this one? - No way. While yes, the proverbial wisdom is true for building muscle, it\u0026rsquo;s also true for systems\nIn 2011, Netflix captured the DevOps world\u0026rsquo;s attention when they released the Chaos Monkey\nI remember that it was a really radical approach at the time. Their goal was to improve service stability and increase fault tolerance by injecting failure into the system. The thinking here was pretty simple. Netflix allowed developers to have autonomy in architecting and building services however they wanted. So to make sure that they were building services and applications that could handle failures from the cloud provider, they would randomly delete services during normal business hours. This has become part of the Netflix culture, and they\u0026rsquo;ve released an entire genre of open-source tooling around this that they brand as the Simian Army. Other tools in the Simian Army include Chaos Gorilla, which will terminate entire Amazon region, and Latency Monkey, which adds in artificial delays to simulate service degradation These tools function as adding deliberate chaos and adversity into the systems. These tools force developers to create services that can withstand this adversity. One of the recommended ways to do this is to think in terms of chaos experiments. - You make a hypothesis, and then take observations to prove whether your hypothesis was correct or not. - Yeah, you could easily call what Chaos Monkey did a chaos experiment Example: I remember once working on a project, and we were building an API that accepted design files, and then they would get processed on the back end. Most of these files were pretty small. So I asked the developer, hey, what if I sent you a two gigabyte file? - Oh no. - Yeah, we did it, you know, and I was able to crash the whole service as it caused a cascading failure, where one node would fail, and then the next node would pick it up and then that would fail, and then the next. We found several bugs that day and implemented a circuit breaker pattern. Chaos engineering is best done in this fashion of experimentation.\nChaos engineering is a new discipline, and it\u0026rsquo;s one that can really pay off. - Right, you know, how often does Netflix go down? It\u0026rsquo;s not that often. - Being mean to your code and doing hard things more often actually does make your service more resilient and more reliable for your customers.\nSRE Organizaion 1. Organizing SREs Remember Conway\u0026rsquo;s law, how you organize definitely has an impact on how your systems turn out. Here\u0026rsquo;s some models we\u0026rsquo;ve seen work to organize for this approach. At one company, we actually tried a couple different models.\nSo we started with the traditional separate ops team model. It\u0026rsquo;s very hard to break traditional expectations while maintaining this model. People love to assign work tickets to the group and this organizational silo also forms a barrier to collaboration. And it\u0026rsquo;s probably even harder to break the habits of operation engineers without a change. So then we declared the death of the ops team and changed to embedding operations engineers into every product team but still reporting up to a central manager. This lets the team work at its own pace without being blocked on a central team and injects them with some operational knowledge. So which setup is best? - That\u0026rsquo;s a trick question. There\u0026rsquo;s no single best. There\u0026rsquo;s what works best given the constraints that you have. But don\u0026rsquo;t artificially limit yourself. You can iterate towards a more successful model no matter where you start out originally\n2. The softer side of SRE The softer side of SRE\nTime Management for System Administrators by Thomas Limoncelli – http://shop.oreilly.com/product/9780596007836.do\nPragmatic Thinking and Learning by Andy Hunt – https://pragprog.com/book/ahptl/pragmatic-thinking-and-learning\nConclusion Learn links: https://www.linkedin.com/learning/devops-foundations-site-reliability-engineering/the-softer-side-of-sre-14766859?resume=false\u0026u=103729754\nRelease It! by Michael Nygard – https://pragprog.com/book/mnee2/release-it-second-edition\nOther links: https://pragprog.com/titles/mnee2/release-it-second-edition/ Site Reliability Engineering: https://sre.google/books/\n"
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.3-github/5.3.1-workflow/",
	"title": "Workflow",
	"tags": [],
	"description": "",
	"content": "Create a workflows Workflows define the event that triggers actions Workflows define with actions to run Repositories can contain multiple workflows Workflows are stored in a directory name .github/workflows\nmkdir -p .github/workflows -p option will create both directories at the same time\nCreate a workflow:\ncd .github/workflows\rvim first.yml first.yml\nname: first\ron: push Add jobs and steop to a workflow\nname: first\ron: push\rjobs:\rjob1:\rname: First Job\rruns-on: ubuntu-latest\rsteps:\r- name: Step one\r- name: Step two\rjob2:\rname: Second Job\rruns-on: windows-latest\rsteps:\r- name: Step one\r- name: Step two Adding an Actions\nuses: Excute an action in the operating system Adding a command\nRun: Excute commands in the operating system\u0026rsquo;s shell Bash: Default shell for Ubuntu, macOS name: first\ron: push\rjobs:\rjob1:\rname: First Job\rruns-on: ubuntu-latest\rsteps:\r- name: Step one\ruses: actions/checkout@v2 #name \u0026amp;tag\r- name: Step two\rrun: env | sort\rjob2:\rname: Second Job\rruns-on: windows-latest\rsteps:\r- name: Step one\ruses: actions/checkout@v2 #name \u0026amp;tag\r- name: Step two\rrun: \u0026#34;Get-ChildItem Env: | Sort-Object Name\u0026#34; Run a workflow\ngit add first.yml\rgit commit -m \u0026#34;First commit\u0026#34;\rgit push Adding Dependencies\nneeds: Identifies one or more jobs that must complete successfully before a job will run Add conditionals Workflow and Action Limitations\n1000 API requests per hour Acction cant trigger other workflows Logs are limited to 64KB Exceeding limits can cause Job queueing Failed jobs Limits are subject to change Using Actions Use an action from the Marketplace\nMaketplace : Python Syntax Checker (copy \u0026amp; paste to .yml file)\nPython file: 100daysofcode.py\nfrom datetime import date start = date(2020, 1, 1) today = date.today() delta = today - start if (delta.days \u0026lt; 101): print(\u0026#34;Today is day {}\u0026#34;.format(delta.days)) else: print(\u0026#39;100 Days of Code sprint has ended\u0026#39;) Github action file: syntax-check.yml name: Python application on: [push] jobs: build: runs-on: ubuntu-latest steps: - name: Check out the code uses: actions/checkout@v2 - name: Python Syntax Checker uses: cclauss/Find-Python-syntax-errors-action@v0.2.0 Workflow: Use an action from a repository\nActions in the workflow\u0026rsquo;s repo Actions in any public repository Docker images from an image registry Use an Action in the same Repo Specify any path relative to the repository root steps: - uses: \u0026#34;./.github/action1\u0026#34; Use an Action in a Different repo Specify the repo owner\u0026rsquo;s user ID, the repo, and a reference steps: - uses: \u0026#34;user/reoi@ref\u0026#34; Reference can be a branch, tag, or SHA steps: - uses: \u0026#34;user/repo/path@ref\u0026#34; steps: - name: Octocat\u0026#39;s Cool Action uses: \u0026#34;octocat/my-cool-action@develop\u0026#34; Use an Action from an Image Repo\nSpecify the \u0026ldquo;docker://\u0026rdquo; path to the image and tag steps:\r- uses: \u0026#34;docker://image:tag\u0026#34; Docker Hub is used by defacult: https://hub.docker.com steps:\r- uses: \u0026#34;docker://python:3.9\u0026#34; steps:\r- uses: \u0026#34;docker://host/image:tag\u0026#34; Passing Arguments to an Action\nSteps use the attribute to pass arguments\nCreate a new block for mapping arguments to unputs\nuses: {github account}/{action name}\rwith:\rkey:value\rkey:value steps:\r- name: Checkout the code\ruses: actions/checkout@v2\rwith:\rrepository: apache/tomcat\rref: master\rpath: ./tomcat Using environment variables\nDynamimc key value pairs stored in memory Injected at runtime Case sensitive Defining Environment Variables Use the env attribute Defined in: Workflows Jobs Steps Accessing Environment Variables Shell variable syntax\nBash (linux/macOS) $VARIABLE_NAME PowerShell (Windows) $Env:VARIABLE_NAME Variable is read from the shell\nYAML syntax\n${{ env.VARIABLE_NAME }} Variable is read from the shell Variable can be used in workflow configuration\nUsing Secrets\nStored as encrypted environment variables Cant be viewed or edited Workflows can have up to 100 secrets Secrets limited to 64kb Larger secrets can be encrypted and stored in the repo Accessing Secrets\nUse the secrets workflow context ${{ secretts.SECRET_NAME }} Must be explicity passed to a step or action secret.yml\nname: secrets on: [push] jobs: build: runs-on: ubuntu-latest steps: - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v1 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ${{ secrets.AWS_REGION }} - name: List S3 Buckets run: aws s3api list-buckets Using artifacts\nData preserved from a workflow\nFiles or collection of files\nCompiled binaries Archives Test results Log files Pass data between workflow jobs\nJob 1 - Create and upload artifact Job 2 Wait for Job 1 to complete Download and use artifact Can only be uploaded by a workflow\nactions/upload-artifact Can only downloaded by the uploading workflow\nManual downloads\nFree accounts get 500MB for storage\nStored for 90 days\nFor pull requests, retention period resets on pushes\nartifact.yml\nhello-server.go\ntest.sh\nManage pull requests\nPull requests Pull facilitated conversations around code Merge code from one branch to another branch Merge code from a cloned repo into the original repo Allows for code reviews Automatin Pull-Request Merging Automatic approve and merge PRs based on criteria Run automated tests to check the code in the PR Check the username that submitted the PR Approve and merge the PR Delete the branch associated with the PR Developing a CI/CD Workflow Continous Integration\nFind and resolve problems early Work locally and then commit to repo Continous Delivery and Deployment\nCompiled into artifacts Stored Additional tests Ready for deployment Deploy to live environments Configure files link\nLinting and Unit test\nLinting: Enfore coding standards Improve code quality Catch errors early in the design cycle Unit Tests: First tests run Checks code at the component level Exposes problems closer to the code Fast running Check: pipeline.yml\nname: Pipeline on: [push] env: APPLICATION_NAME: app2 jobs: lint: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v1 - name: Setup Python environment uses: actions/setup-python@v1.1.1 - name: Install requirements run: pip install --quiet --requirement requirements.txt - name: Lint code run: | flake8 --ignore=E501,E231 *.py pylint --disable=C0301 --disable=C0326 *.py - name: Run unit tests run: | python -m unittest --verbose --failfast "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.2-gitlab/6.2.1-workflow/",
	"title": "Workflow",
	"tags": [],
	"description": "",
	"content": "Gitlab-Server Repo: shoeshop\nNew branch:\nBranch name: develop -\u0026gt; Create from: main Branch name: feature/frontend/login -\u0026gt; Create from: develop Branch name: feature/backend/login -\u0026gt; Create from: develop Set permission approve : http://gitlab.local.test/shoeshop/shoeshop/-/settings/repository\nProtected branches :\nBranch: Allowed to merge develop\t- Maintainers - Maintainers main\t- Maintainers\t- No one Default branch : develop (for Developer Teams)\nNew branch:\nBranch name: staging -\u0026gt; Create from: develop Protected branches : Branch: Allowed to merge staging\t- Maintainers - Maintainers Permission testing Edit repo: http://gitlab.local.test/shoeshop/shoeshop/-/new/develop/ User: tuanta New file: config.txt Commit: config(database): modify connection Target branch: develop -\u0026gt; OK Edit repo: http://gitlab.local.test/shoeshop/shoeshop/-/new/main/ User: tuanta New file: config.txt Commit: config(database): modify connection ISSUE: -\u0026gt; You are not allowed to push into this branch "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.6-docker/6.6.1-workflow/",
	"title": "Workflow",
	"tags": [],
	"description": "",
	"content": "Overview Docker backend: Chọn image đúng version dự án Chọn image từ các nguồn offical, verified, sponsored Chọn base image là alpine (nhẹ) Project folder: /data/shoeshop\n1.Create Database:\napt install mariadb-server systemctl stop mariadb apt install net-tools nano /etc/mysql/mariadb.conf.d/50-server.cnf bin-address = 0.0.0.0 systemctl restart mariadb netstat -tlpun 2.Database configuration:\nmysql -u root\nshow databases; create database shoeshop; create user \u0026lsquo;shoeshop\u0026rsquo;@\u0026rsquo;%\u0026rsquo; identified by \u0026lsquo;shoeshop\u0026rsquo;; grant all privileges on shoeshop.* to \u0026lsquo;shoeshop\u0026rsquo;@\u0026rsquo;%\u0026rsquo;; flush privileges; exit mysql -h 192.168.33.100 -P 3306 -u shoeshop -p\nEnter password:\tshoeshop show databases; use shoeshop; show tables; source /data/shoeshop/shoe_shopdb.sql show tables; exit nano src/main/resources/application.properties\nspring.datasource.url=jdbc:mysql://192.168.33.100:3306/shoeshop spring.datasource.username=shoeshop spring.datasource.password=shoeshop vi Dockerfile\n## build stage ## FROM maven:3.5.3-jdk-8-alpine as build WORKDIR /app COPY . . RUN mvn install -DskipTest=true ## run stage ## FROM amazoncorretto:8u402-alpine-jre WORKDIR /run COPY --from=build /app/target/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar /run/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar EXPOSE 8080 ENTRYPOINT java -jar /run/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar 3.Run Project:\ndocker build -t shoeshop:v1 . docker run \u0026ndash;name shoeshop -dp 8899:8080 shoeshop:v1 docker images docker ps -a Check: http://192.168.33.100:8899\n4.Advanced: nano Dockerfile-v2\n\u0026#34;## build stage ## FROM maven:3.5.3-jdk-8-alpine as build WORKDIR /app COPY . . RUN mvn install -DskipTest=true ## run stage ## FROM alpine:3.19 RUN apk add openjdk8 WORKDIR /run COPY --from=build /app/target/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar /run/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar EXPOSE 8080 ENTRYPOINT java -jar /run/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar\u0026#34; 5.Advanced: User Permissons\nUser: Shoeshop\nadduser shoeshop chown -R shoeshop. /data/shoeshop/ chmod -R 750 /data/shoeshop/ Dockerfile-v3\n\u0026#34;## build stage ## FROM maven:3.5.3-jdk-8-alpine as build WORKDIR /app COPY . . RUN mvn install -DskipTest=true ## run stage ## FROM alpine:3.19 RUN adduser -D shoeshop RUN apk add openjdk8 WORKDIR /run COPY --from=build /app/target/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar /run/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar RUN chown -R shoeshop:shoeshop /run USER shoeshop EXPOSE 8080 ENTRYPOINT java -jar /run/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar\u0026#34; Run project:\ndocker build -t shoeshop:v3 -f Dockerfile-v3 . docker run \u0026ndash;name shoeshop-v3 -dp 7777:8080 shoeshop:v3 docker logs shoeshop-v3 docker exec -it shoeshop-v3 sh ls -l ps -ef| grep shoe Docker Frotend 1.Project folder:\nmkdir /data/todolist apt insall unzip unzip todolist.zip cdp -rf todolist/* /data/todolist cd /data/todolist Insall npm sudo apt install npm -y Create Dockerfile ## build stage ## FROM node:18.18-alpine as build WORKDIR /app COPY . . RUN npm install RUN npm run build ## run stage ## FROM nginx:alpine RUN apk add openjdk8 WORKDIR /run COPY --from=build /app/dist /usr/share/nginx/html EXPOSE 8080 CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;] Run Project docker build -t todolist:v1 . docker run \u0026ndash;name todolist-v1 -dp 6868:80 todolist:v1 http://localhost:6868 Docker Volume, Compose, Network Docker Volume\ndocker run \u0026ndash;rm -v \u0026lsquo;pwd\u0026rsquo;:/app \u0026ndash;workdir=\u0026quot;/app\u0026quot; mnv install -DskipTests=true mkdir -p /db/mariadb-1 docker run -v /db/mariadb-1/:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=root -p 3307:3306 \u0026ndash;name mariadb-1 -d mariadb:10.6 docker ps netstat -tlpn mysql -h 192.168.1.110 -P 3307 -u root -p MariaDB [(none)]\u0026gt; show databases; create database demo show databases; exit ls /db/mariadb-1/ Delete container images and run docker again Docker Compose\nvi docker-compose.yml version: \u0026#39;3.8\u0026#39; services: db1: image: mariadb:10.6 volumes: - /db/mariadb-1:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: root ports: - \u0026#34;3307:3306\u0026#34; container_name: mariadb-1 restart: always app-backend: image: shoeshop:v3 ports: - \u0026#34;8081:8080\u0026#34; container_name: shoeshop-1 restart: always docker-compose up -d docker-compose ps Docker Network docker inspect shoeshop-1 docker exec -it shoeshop-1 sh whoami docker exec -it mariadb-1 sh whoami apt update apt install iputils-ping -y ping 172.18.0.3 Docker Registry "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.3-monitor/6.3.1-zabbix/",
	"title": "Zabbix",
	"tags": [],
	"description": "",
	"content": "Overview Zabbix is an open-source monitoring software tool designed to track the status of various network services, servers, and other network hardware. It provides real-time monitoring, alerting, and visualization of metrics such as CPU load, network utilization, disk space, and more. Configuration Zabbix Server Configuration: collects and processes data Install Repo wget https://repo.zabbix.com/zabbix/6.4/ubuntu/pool/main/z/zabbix-release/zabbix-release_6.4-1+ubuntu20.04_all.deb dpkg -i zabbix-release_6.4-1+ubuntu20.04_all.deb apt update -y Install Zabbix server, frontend, agent apt install zabbix-server-mysql zabbix-frontend-php zabbix-apache-conf zabbix-sql-scripts zabbix-agent -y Create initial database apt install software-properties-common -y apt update -y apt install -y mariadb-server-10.6 mariadb-client-10.6 mariadb-common systemctl start mariadb systemctl status mariadb Configuration Database : mysql -u root create database zabbix character set utf8mb4 collate utf8mb4_bin; create user zabbix@localhost identified by \u0026#39;password\u0026#39;; grant all privileges on zabbix.* to zabbix@localhost; set global log_bin_trust_function_creators = 1; quit; cat /usr/share/zabbix-sql-scripts/mysql/server.sql.gz | mysql --default-character-set=utf8mb4 -uzabbix -p zabbix mysql -u root set global log_bin_trust_function_creators = 0; exit Configure the db for Zabbix server vi /etc/zabbix/zabbix_server.conf DBPassword=password Start Zabbix server and agent processes systemctl restart zabbix-server zabbix-agent apache2 systemctl enable zabbix-server zabbix-agent apache2 Zabbix Agent Configuration: run on the monitored devices and send data to the server\nInstallation wget https://repo.zabbix.com/zabbix/6.4/ubuntu/pool/main/z/zabbix-release/zabbix-release_6.4-1+ubuntu20.04_all.deb dpkg -i zabbix-release_6.4-1+ubuntu20.04_all.deb apt update -y apt install zabbix-agent Configuration : vi /etc/zabbix/zabbix_agentd.conf Server=\u0026lt;zabbix dns\u0026gt; ServerActive=\u0026lt;zabbix dns\u0026gt; Hostname=\u0026lt;zabbix server ip\u0026gt; systemctl restart zabbix-agent Add Host to Zabbix Server\nLogin Add Host Host - Create host Host name:\t\u0026lt;Agent + IP\u0026gt; Templates:\tLinux by Zabbix agent Host groups: Linux servers Interfaces: Agent - IP: \u0026lt;Agent IP\u0026gt; -\u0026gt; Add Create Item \u0026amp; Trigger\nItem: Name:\tApp service on server \u0026lt;Host Agent IP\u0026gt;. running on port 8080 is unavaiilable\tType:\tZabbix agent\tKey:\tnet.tcp.listen[8080]\t-\u0026gt; Select\tnet.tcp.listen[port] Type of information:\tNumeric (unsigned)\tHost interface:\t\u0026lt;agent IP\u0026gt; :10050\tUpdate interval\t10s\t-\u0026gt; TEST -\u0026gt; OK Trigger: Name:\tApp service on server 192.168.1.110. running on port 8080 is unavaiilable Severity:\tHigh Expresstion:\tItem: App service on server \u0026lt;Host Agent IP\u0026gt;. running on port 8080 is unavaiilable Result = 0 -\u0026gt; Add Dashboard Overview:\nHiKVision APIs for Monitoring: Overview: Monitoring Hikvision devices using Zabbix involves leveraging Zabbix\u0026rsquo;s capabilities to communicate with Hikvision APIs. "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.1-jenkins/6.1.2-advanced/",
	"title": "Advanced",
	"tags": [],
	"description": "",
	"content": "Jenkins advanced is processing \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.6-docker/6.6.2-advanced/",
	"title": "Advanced",
	"tags": [],
	"description": "",
	"content": "Processing \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/thedevops/2-prepair/2.2-aws/",
	"title": "AWS",
	"tags": [],
	"description": "",
	"content": "Amazon Web Services (AWS) is a comprehensive and widely used cloud computing platform provided by Amazon. It offers a vast array of services, allowing individuals and businesses to build and deploy scalable applications and services without the need to invest in physical infrastructure\nOverview AWS CLI : The AWS Command Line Interface (CLI) is a powerful tool provided by Amazon Web Services (AWS) that allows you to interact with AWS services directly from your command line or terminal It provides a convenient and scriptable way to manage your AWS resources without needing to use the AWS Management Console Configuration Prepair and run docker compose file\ndocker-compose run --rm aws --version AWS: Create keypair to access AWS Instances: tf-cli-keypair.pem\ndocker-compose run --rm aws ec2 create-key-pair --key-name tf-cli-keypair --query \u0026#39;KeyMaterial\u0026#39; --output text \u0026gt; tf-cli-keypair.pem Create AWS Account for Terraform use AWS CLI: tf-cli\ndocker-compose run --rm aws iam create-user --user-name tf-cli AWS Checking keypair: Create Access Key \u0026amp; export to local\ndocker-compose run --rm aws iam create-access-key --user-name tf-cli \u0026gt; tf_cli-access_key.json Create policy and configure to allow access EC2 and Limit Region\nCreate a custom policy file: ec2-limited-access-policy.json {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: \u0026#34;ec2:*\u0026#34;,\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;,\r\u0026#34;Condition\u0026#34;: {\r\u0026#34;StringEquals\u0026#34;: {\r\u0026#34;ec2:Region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;\r}\r}\r}\r]\r} Create a IAM policy: EC2FullAccessAPSouthEast1 docker-compose run --rm aws iam create-policy --policy-name EC2FullAccessAPSouthEast1 --policy-document file://ec2-limited-access-policy.json Attach the Policy to the IAM User: (tf-cli) docker-compose run --rm aws iam attach-user-policy --user-name tf-cli --policy-arn arn:aws:iam::637423373411:policy/EC2FullAccessAPSouthEast1 AWS Checking User: "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.5-linux/6.5.2-backend/",
	"title": "Backend",
	"tags": [],
	"description": "",
	"content": "Project workflow Tools Edit \u0026amp; configure files Install \u0026amp; configure Database Build Run Check status Configuration Source: shoeshop-ecommerce.zip\nUser: shoeshop\nJava project folder: /projects/shoeshop\nFolder and User permisson: adduser shoeshop\rchown -R shoeshop. /projects/shoeshop/\rchmod -R 750 /projects/shoeshop/ Requires tools: https://spring.io/guides/gs/maven Maven: defined with an XML file named pom.xml\nJava:\nChecking require java version in pom.xml -\u0026gt; java.version=1.8 Install:\napt install openjdk-17-jdk openjdk-17-jre\rapt install maven\rjava -version\rmvn -v Configuring Database Spring boot: https://www.baeldung.com/spring-boot-configure-data-source-programmatic\nDataSource: application.properties Install Database: MariaDB apt install mariadb-server\rnetstat -tlpun Database should install in difference Server\nConfigure DB: systemctl stop mariadb\rls /etc/mysql/ nano /etc/mysql/mariadb.conf.d/50-server.cnf\nbin-address = 0.0.0.0 systemctl restart mariadb\rnetstat -tlpun Create Database: import shoe_shopdb.sql to MariaDB mysql -u root\nshow databases: show databases; Create datatabse: Database and User user: shoeshop @%: can access all server identified: password grant: permission for shoeshop apply to all resource flush privileges: save changes show databases;\rcreate database shoeshop;\rcreate user \u0026#39;shoeshop\u0026#39;@%\u0026#39;identified by \u0026#39;shoeshop\u0026#39;;\rgrant all privileges on shoeshop.* to \u0026#39;shoeshop\u0026#39;@\u0026#39;%\u0026#39;;\rflush privileges;\rexit Login user shoeshop: mysql -h 192.168.33.110 -P 3306 -u shoeshop h: host p: port u: user show databases;\ruse shoeshop;\rshow tables;\rsource /projects/shoeshop/shoeshopdb.sql\rshow tables;\rexit vi /src/main/resources/application.properties\nspring.datasource.url=jdbc:mysql://192.168.33.110:3306/shoeshop\rspring.datasource.username=shoeshop\rspring.datasource.password=shoeshop mvn install -DskipTest=true\nls target/\njava -jar target/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar nohup java -jar target/shoe-ShoppingCart-0.0.1-SNAPSHOT.jar 2\u0026gt;\u0026amp;1 \u0026amp; tail -f nohup.out\nCheck: http://192.168.33.110:8080 "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.3-github/5.3.2-challenge/",
	"title": "Challenge",
	"tags": [],
	"description": "",
	"content": "Chalenge: Develop a CI/CD pipeline Create a mew repo and add the exercise files Edit the workflow to place the jobs in order Configure three environments: Development Staging Production Use continuos deployment for the devlopment and staging environments Protect deployments to the production environment with a review Update the summary to indicate all jobs Solution\nTimelines \u0026amp; Get files\nChalenge: Develop a container image Repo 1:\nReuseable Python intefraton workflow Repo 2:\nAdd the code for the API Use the integration workflow from Repo 1 Build and publish a container image Solution\nTimelines \u0026amp; Get files\nChallenge: Develop a CI/CD workflow User a Stater Workflow\nCreate a mew repo and add the exercise files Create a starter workflow Run the workflow and review the output Fix any errors in the code base Add a test report summary Solution\nTimelines \u0026amp; Get files\ntest_pandas_version.py\nrequirements.txt\nActions - Choose a workflow - Python application: python-app.yml -\u0026gt; Commit changes\nCheck Acctions -\u0026gt; ISSUE\nFix: requirements.txt: =pandas==1.5.3 Update the workflow to add a summary of the tests being run. \u0026hellip;.-\u0026gt; follow timelines :)\nResult: Challenge: Configure a self-hosted runner with a lable Private repository Add a new runner to the repository Configure the runner with the label project-alpha Run a job with the project-alpha label Use a cloud-based virtual machine if possible Consider using a free tier for access to resources Challenge: Public and Use a Container image in a workflow Create a new repo Add the exercise files Use the workflow suggested by Github Actions Add a workflow dishpath trigger Add a new job to run the test Run the image and check ouput docker run ghcr.io/repo/image:main Test for the word \u0026ldquo;container\u0026rdquo; in the output Use a CLI tool like grep Challenge: Use a matrix strategy to test an application Create a new repo Add the application files Add the workflow file Update the workflow to use a matrix strategy Add matrix keys and values: Platform: Ubuntu, macOS, Windows Version: \u0026ldquo;3.10\u0026rdquo;.\u0026ldquo;3.9\u0026rdquo;,\u0026ldquo;3.8\u0026rdquo;,\u0026ldquo;3.7\u0026rdquo; Update the runs-on and setup-python configurations Add an include block: Python version \u0026ldquo;3.6\u0026rdquo; ubuntu-latest Add an exclude block: Python version \u0026ldquo;3.10\u0026rdquo; windows-latest Challenge: Create a custom action Custom Action:\nDockerfile Script Workflow:\nTrigger on push events Use the custom action Solution:\nCreate Dockerfile FROM ubuntu\rCOPY entrypoint.sh /entrypoint.sh\rRUN chmod +x /entrypoint.sh\rENTRYPOINT [\u0026#34;/entrypoint.sh\u0026#34;] Create entrypoint.sh #!/bin/bash\recho \u0026#34;This is my custom action! :D\u0026#34; Create a workflow: .github/workflows/custom.yml name: Custom\ron: push\rjobs:\rmain:\rruns-on: ubuntu-latest\rsteps:\r- uses: actions/checkout@v2\r- uses: nonotnonez/thegithub@main Check Actions: Challenge: Develop a CI/CD pipeline for a Python Script Job 1: Test\nCheck out the repo Run the script:python hello.py Job 2: Build\nDepends on Job 1 Check out the repo Create an artifact Solution:\nCreate repo: pipeline\nadd a README file Create python file: hello.py\nprint(\u0026#34;hello, world\u0026#34;) Add a workflow file: .github/workflows/pipeline.yml name: Python Pipeline on: push jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - run: python hello.py build: needs: [test] runs-on: ubuntu-latest steps: - uses: actions/checkout@v2 - uses: actions/upload-artifact@v2 with: name: hello path: . Challenge: Develop a workflow that creates an artifact Workflow file in a new repo Push trigger Environment variable for the artifact name One job with two steps actions/checkout actions/upload-artifact Solution:\nCreate repo: artifact add a README file Add a workflow file: .github/workflows/artifact.yml name: Artifact\ron: [push]\renv:\rARTIFACT_NAME: myartifact\rjobs:\rmain:\rruns-on: ubuntu-latest\rsteps:\r- name: Check out the code\ruses: actions/checkout@v2\r- name: Upload the artifact\ruses: actions/upload-artifact@v2\rwith:\rname: ${{ env.ARTIFACT_NAME }}\rpath: . Commit a new file Checking Actions - Artifact file Challenge: Develop a complex workflow Push trigger\nMultiple runners\nJob dependencies\nWorkflow file in a repo\nFour jobs\nPrint the date\nJob 1- ubuntu-latest Job 2- windows-latest Job 3- macos-latest Job 4- depends on jobs 1, 2, and 3 Solution\n1.Create new repo: complex-workflow\n2.Create a new fie: .github/workflows/complex.yml\nname: Complex\ron: push\rjobs:\rubuntu:\rruns-on: ubuntu-latest\rsteps:\r- run: date\rwindows:\rruns-on: windows-latest\rsteps:\r- run: date\rmacos:\rruns-on: macos-latest\rsteps:\r- run: date\rdepends:\rneeds: [ubuntu, windows, macos]\rruns-on: macos-latest\rsteps:\r- run: date 3.Commit file\n4.Actions check: - All workflows - complex\n"
},
{
	"uri": "http://localhost:1313/thedevops/3-config/3.2-cicd/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "Updating \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.2-gitlab/",
	"title": "Gitlabs",
	"tags": [],
	"description": "",
	"content": "Overview GitLab is a web-based DevOps lifecycle tool that provides a Git repository manager providing wiki, issue-tracking, and CI/CD pipeline features, using an open-source license. It offers functionalities similar to GitHub but with additional features focused on the entire DevOps lifecycle.\nConfiguration Installation curl -s https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash\rsudo apt-get install gitlab-ee=14.4.1-ee.0 Hostname:\nvi /etc/hosts 192.168.33.110 gitlab.local.test vi /etc/gitlab/gitlab.rb external_url \u0026#39;http://gitlab.local.test\u0026#39;\rgitlab-ctl reconfigure 2.Login : http://gitlab.local.test\nUser: root - Password: cat /etc/gitlab/initial_root_password\nView settings:\nUncheck: Siginup enabled \u0026amp; Required admin \u0026hellip; Setting - CI/CD - Continuos Integration and Deployment - Expand : Uncheck: Default to Auto DevOps Change password root user:\nUser: root - Edit Profiles - Password Create User:\nMenu - Admin New User: tuanta Access level: admin New User: dev1 Access level: regular Project process: Gitlab-Server: Menu - Group - Create Group - Create Group\nGroup name: shoeshop Private Role: Devops Engineer New project:\nProject name: shoeshop Uncheck: Initialize repository with a README Invited members: tuanta: maintainer dev1: develop Git configure: lab-server: 192.168.33.110\nLab-Server Folder: mkdir /data\ncd /data\nGit config:\ngit config \u0026ndash;global user.name \u0026ldquo;tuanta\u0026rdquo; git config \u0026ndash;global user.email \u0026ldquo;tuanta@gitlocal.test\u0026rdquo; Clone source: git clone http://gitlab.local.test/shoeshop/shoeshop.git\nEnter user/passsword: tuanta Copy Source Code and Add permission to user\ncp -rf shoeshop/* /data/shoeshop/ cd /data/shoeshop Upload to repo:\ngit status git checkout -b main git add . git commit -m \u0026ldquo;feat(project): create base projects\u0026rdquo; git push -f origin git push \u0026ndash;set-upstream origin main Enter user/password: tuanta CICD Apply Gitlab-server\nEdit README.md Commit: doc(sys desc): change system title Lab-server\nGit pull d- Enter user/password: tuanta Content Workflow Runner CICD "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.2-iac/",
	"title": "IAC",
	"tags": [],
	"description": "",
	"content": "DevOps Foundations: Infrastructure as Code Bringing It All Together What you\u0026rsquo;ll need: MacOS, Linux, or Windows with WSL2 Amazon Web Services account "
},
{
	"uri": "http://localhost:1313/thedevops/2-prepair/",
	"title": "Prepairation",
	"tags": [],
	"description": "",
	"content": "Diagram Workshop-0001 In this lab, we will be automate deploy a Server in AWS Cloud with tools: AWS CLI, Docker Compose, Terraform. Following this link Workshop-0002 CICD is processing \u0026hellip; Following this link Contents Docker AWS Teraform Git Local "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.3-monitor/6.3.2-prometheus/",
	"title": "Prometheus",
	"tags": [],
	"description": "",
	"content": "Overview Prometheus is an open-source monitoring and alerting toolkit originally built at SoundCloud. It provides a robust monitoring and alerting solution for modern, cloud-native environments, enabling developers and operators to gain insights into the performance and health of their systems.\nConfiguration "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.2-gitlab/6.2.2-runner/",
	"title": "Runner",
	"tags": [],
	"description": "",
	"content": "GitLab Runner is a lightweight agent that runs jobs defined in your GitLab CI/CD pipelines. It works by listening for jobs that need to be executed and then runs them in a specified environment.\nLab-Server Install sudo apt-get update\rcurl -L https://packages.gitlab.com/install/repositories/runner/gitlab-runner/script.deb.sh | sudo bash\rsudo apt install gitlab-runner\rgitlab-runner -v Edit hostname: vi /etc/hosts gitlab.local.test \u0026lt;ip\u0026gt; Configuration: gitlab-runner register Enter the GitLab instance: http://gitlab.local.test/\rEnter the registration token:\rEnter a description for the runner: lab-server\rEnter tags for the runner: lab-server\rEnter optional maintenance note for the runner: \u0026lt;enter\u0026gt;\rexecutor: shell vi /etc/gitlab-runner/config.toml\nconcurrent = 4 -\u0026gt; use 4 robot runner Start Runner nohup gitlab-runner run --working-directory /home/gitlab-runner/ --config /etc/gitlab-runner/config.toml --service gitlab-runner --user gitlab-runner 2\u0026gt;\u0026amp;1 \u0026amp;\rps -ef | grep gitlab-runner Gitlab-Server CICD Settings: http://gitlab.local.test/shoeshop/shoeshop/-/settings/ci_cd\nEdit: http://gitlab.local.test/shoeshop/shoeshop/-/runners/2/edit Uncheck : When a runner is locked \u0026hellip;. Branch: develop - Allowed to push: Maintainers Create CICD file: .gitlab-ci.yml \u0026amp; Commit: Configure(pipeline): add runner\nstages:\r- build\r- deploy\r- checking\rbuild:\rstage: build\rscript:\r- whoami\r- pwd\r- ls\rtags:\r- jenkins-server Checking Gitlab-Server:\nJobs Lab-Server:\nFolder: -\u0026gt; /home/gitlab-runner/builds/1fijYgCS/0/shoeshop/shoeshop "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.3-github/5.3.3-advanced/",
	"title": "Advanced",
	"tags": [],
	"description": "",
	"content": "Overview Caching:\nService containers:\nScheduled Triggers:\nComposite actions: Manual approvals and environments\nMatrix Strategy Publish Packages Self-Hosted Runners "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.2-gitlab/6.2.3-cicd/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "Updating \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/thedevops/3-config/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": " The Projects for the workshop \u0026hellip;.. Content IaC CICD Labs "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.3-github/",
	"title": "GitHub",
	"tags": [],
	"description": "",
	"content": "Home | Workflow | Challenge | Advanced | Pages\nLearning GitHub Actions Source: Linkedin Learning GitHub Actions is a continuous integration tool that allows developers to automate tasks for their web projects. In this course, learn how to use this powerful tool to build workflows triggered by events, develop a continuous integration and continuous delivery (CI/CD) pipeline, and create custom actions. What you should know: Github Github account Create a repository on GitHub.com Create and manage a repository branch Commit code changes to a repositoy Work with pull requests YAML Working with YAML Docker and Scripting Use a Dockerfile to create containers Shell scripting Scripting a higher-level language Introduction Create a repo\nCreate accesskey: Clone repo to local:\ngit clone https://\u0026lt;access token\u0026gt;U@github.com/nonotnonez/thegithub.git Create a README.md file Copy Source and Push to git repo:\ngit add .\rgit commit -m \u0026#39;first check in\u0026#39;\rgit push Your first action Setup Github actions: https://github.com/nonotnonez/thegithub/actions/new\nSimple workflow -\u0026gt; Configure Change name blank.yml to hello.yml and Commit changes Click Actions to see the results Workflow and actions attributes\nname: The name of the workflow Not required on: The Github event that triggers the workflow Required On Events: Repository events push pull_request release Webhooks branch creation issues members Scheduled Cron format Jobs: Workflows must have at least one job Each job must have a identifier Must start with a letter or underscore Can only contain alphanumeric character,-,or_ runs-on: The type of machine needed to run the job Runners: Windows Server 2019 Ubuntu 18.04 macOS Catalina 10.15 Self-hosted runners steps List of actions or commands Access to the file system Each step runs in its own process uses Identifies an action to use Defines the location of that action run runs commands in the vitual environment\u0026rsquo;s shell name an optional identifier for the step "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.3-monitor/6.3.3-grafana/",
	"title": "Grafana",
	"tags": [],
	"description": "",
	"content": "Overview Grafana is an open-source analytics and monitoring platform designed to help you visualize and understand your metrics. It allows you to query, visualize, alert on, and understand your metrics no matter where they are stored.\nConfiguration "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.3-monitor/",
	"title": "Monitoring",
	"tags": [],
	"description": "",
	"content": "Monitoring is processing \u0026hellip;.\nContent Zabbix Prometheus Grafana ELK Stack "
},
{
	"uri": "http://localhost:1313/thedevops/2-prepair/2.3-terraform/",
	"title": "Terraform",
	"tags": [],
	"description": "",
	"content": "Terraform is a open-source tool used to build, modify, and version control infratrucrure\nOverview Provider(provider.tf): Enables Terrafrom to interact with cloud providers and other APIs Terraform (versions.tf): Sets version constaints for Terraform and optionally maps provides to a source address and version constaint Variables (variable.tf): Input variables define reusable values and work like function arguments in general-purpose programming languages Resource (main.tf): Resource blocks describe infrastructure objects like VPCs, subnets, route tables, and gateways Data : Data sources allow Terraform to ultilize information form resources that were defined outside of Terraform (or defined a different Terraform configuration) Output: Outputs return structured data form your configuration and work like return values in generaral-purpose programming languages Terraform.tfvars: To set lots of variables, it is more convenient to specify their values in a variable definitions file Command terraform init [options]: command initializes a working directory containing Terraform configuration files. terraform plan [options]: command creates an execution plan, which lets you preview the changes that Terraform plans to make to your infrastructure. terraform apply [options] [plan file]: command executes the actions proposed in a Terraform plan terraform destroy [options]: command is a convenient way to destroy all remote objects managed by a particular Terraform configuration. Run Terraform in containter: Run docker compose:\ndocker-compose run --rm terraform version Run configure:\nProvider (AWS): versions.tf\nterraform {\rrequired_providers {\raws = {\rsource = \u0026#34;hashicorp/aws\u0026#34;\rversion = \u0026#34;~\u0026gt; 4.16\u0026#34;\r}\r}\rrequired_version = \u0026#34;\u0026gt;= 1.2.0\u0026#34;\r} Security credential variables: variables.tf\nvariable \u0026#34;access_key\u0026#34; {\rtype = string\rsensitive = true\r}\rvariable \u0026#34;secret_key\u0026#34; {\rtype = string\rsensitive = true\r}\rvariable \u0026#34;region\u0026#34; {\rtype = string\rdefault = \u0026#34;ap-southeast-1\u0026#34;\r} Terraform init:\ndocker-compose run --rm terraform init "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.3-monitor/6.3.4-elk/",
	"title": "ELK Stack",
	"tags": [],
	"description": "",
	"content": "Overview The ELK Stack stands for Elasticsearch, Logstash, and Kibana, which is a powerful set of open-source tools commonly used for log management and analytics. Elasticsearch: This is a distributed, RESTful search and analytics engine designed for horizontal scalability, reliability, and real-time search capabilities. It\u0026rsquo;s providing storage and indexing functionality for logs and other data. It\u0026rsquo;s highly efficient for searching and analyzing large volumes of data in near real-time. Logstash: Logstash is a data processing pipeline that ingests, transforms, and enriches data before it\u0026rsquo;s indexed into Elasticsearch. It supports a wide range of input sources, including logs, metrics, events, and other structured or unstructured data formats. Logstash can perform various operations on the data, such as parsing, filtering, and formatting, to make it suitable for analysis. Kibana: Kibana is the visualization and user interface component of the ELK Stack. It provides a web interface for users to visualize and explore data stored in Elasticsearch.Kibana offers various visualization options, including charts, graphs, maps, and dashboards, allowing users to create custom visualizations and dashboards to gain insights from their data Configuration Firewall pfSense and ELK Stack Review:\npfelk is a highly customizable open-source tool for ingesting and visualizing your firewall traffic with the full power of Elasticsearch, Logstash and Kibana. Git repo reference: https://github.com/pfelk/pfelk. Use Docker: https://github.com/pfelk/pfelk/blob/main/install/docker.md Logs Sever repair:\nDisabling Swap sudo swapoff -a sudo sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab Configuration Date/Time Zone sudo timedatectl set-timezone Asia/Ho_Chi_Minh Configure Memory sudo sysctl -w vm.max_map_count=262144 sysctl vm.max_map_count Download and Install Docker - Docker Compose apt-get install docker apt -y install docker.io sudo apt-get install docker-compose Download Repo : https://github.com/pfelk/pfelk Configuration\nEdit credentials .env ELASTIC_PASSWORD=changeme KIBANA_PASSWORD=changeme LOGSTASH_PASSWORD=changeme LICENSE=basic Edit logstash configure: etc/logstash/config/logstash.yml Run docker compose: docker-compose up -d Settings:\nLog-Server: Import Template : https://github.com/pfelk/pfelk/tree/main/etc/pfelk/templates pfelk-mappings-ecs pfelk-ilm pfelk Options: pfelk-dhcp; pfelk-nginx Import Dashboard : https://github.com/pfelk/pfelk/tree/main/etc/pfelk/dashboard 22.04-firewall.ndjson Options: 22.01-unbound.ndjson; 22.01-captive.ndjson; v20.2-dhcp.ndjson Firewall-Configure: sent log to log server\nStatus \u0026gt; System Logs \u0026gt; Settings Dashboard view: Documents: https://docs.google.com/spreadsheets/d/1zewwK5ikojziCHTMt8bzRvbOV1ewgV-UjozBhae1-Lw/edit#gid=274683191\n"
},
{
	"uri": "http://localhost:1313/thedevops/2-prepair/2.4-git/",
	"title": "Git",
	"tags": [],
	"description": "",
	"content": "Git is a distributed version control system (DVCS) that helps developers track changes to source code during software development. It allows multiple developers to collaborate on projects simultaneously.\nOverview Github GitHub is a web-based platform built on top of Git, the distributed version control system. It offers a variety of features to help developers collaborate on software projects GitHub provides a platform for hosting Git repositories. Developers can create new repositories to store their code, either publicly (visible to everyone) or privately (accessible only to authorized collaborators) Configuration Create Github Repository and Access\nCreate a public repo: https://github.com/nonotnonez/ws-0001\nCreate Github Access Key : https://github.com/settings/tokens\nName: github_token_ws Expiration:\t90 days Select scopes: repo workflow Clone Source form Github: git clone https://**token**@github.com/NoNotNonez/ws-0001.git\nCopy Source code to Github Repo:\ncd /ws-0001/terraform Create .gitignore:\nto security and prevent important file upload to github Push Source code to Git Repo:\ngit status git add . git commit -m \u0026ldquo;Add Tf source\u0026rdquo; git push "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.3-github/5.3.4-pages/",
	"title": "Github Pages",
	"tags": [],
	"description": "",
	"content": "Craft a Great GitHub Profile Content\nLearning GitHub Pages Home | Workflow | Challenge | Advanced | Pages\nLab Content: https://raybo.org/slides_publish/\nUse Repository: https://github.com/LinkedInLearning/publish-github-2894570\nVsCode: CodeSwing\nAttachments\rDockerfile\r(0 KB)\rdevops.png\r(35 KB)\rCustom page domain: Github - Settings - Pages : Custom domain: raybo.org\nGetting Your Website Online Setting up a static site host Github Pages: https://pages.github.com/ 1- Create repo \u0026amp; Upload files 2- Go to Settings:\nPages:\nCheck website: 3- Create Sub website:\nCreate new repo: test-project Crate a new file: index.html This is my index page! Go to Settings - Pages Build and devployment Source: Deloy form a branch Branch: main -\u0026gt; Save Refesh curent page Check github websites: Reference: https://www.netlify.com Overview: There\u0026rsquo;s a free tier that allows you to host multiple websites, as well as other features, such as rolling back to previous versions It could be connect to ant Git host Can manual deploy : https://app.netlify.com/drop Setting up domain and host with a website builder: https://www.squarespace.com/ Publishing projects on social coding platforms "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.3-github/5.3.5-hugo/",
	"title": "Hugo",
	"tags": [],
	"description": "",
	"content": "Learning Static Site Building with Hugo The Hugo \u0026ldquo;Learn\u0026rdquo; theme is a fantastic choice if you\u0026rsquo;re looking to create a static website, particularly one focused on documentation, tutorials, or knowledge bases\nInstall themes Source : https://github.com/matcornic/hugo-theme-learn\ncd /themes git clone https://github.com/matcornic/hugo-theme-learn.git Basic configuration config.toml (hugo.toml) # Change the default theme to be use when building the site with Hugo theme = \u0026#34;hugo-theme-learn\u0026#34; # For search functionality [outputs] home = [ \u0026#34;HTML\u0026#34;, \u0026#34;RSS\u0026#34;, \u0026#34;JSON\u0026#34;] Create your first chapter page hugo new \u0026ndash;kind chapter basics/_index.md\n+++ title = \u0026#34;Basics\u0026#34; date = 2024-03-26T08:30:26+07:00 weight = 5 chapter = true pre = \u0026#34;\u0026lt;b\u0026gt;X. \u0026lt;/b\u0026gt;\u0026#34; +++ ### Chapter X # Some Chapter title Lorem Ipsum Launching the website locally hugo serve Go to http://localhost:1313 "
},
{
	"uri": "http://localhost:1313/thedevops/4-cleanup/4.1-ec2/",
	"title": "IaC",
	"tags": [],
	"description": "",
	"content": "Clean up resources We will process to clearn up all the resources\nTerraform: Run docker compose:\ndocker-compose run --rm terraform destroy --auto-approve AWS Checking "
},
{
	"uri": "http://localhost:1313/thedevops/4-cleanup/",
	"title": "Resource Cleanup",
	"tags": [],
	"description": "",
	"content": "Clean up resources All processing to clearn up all the resources\nContent IAC CICD "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.4-security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": "Security is processing \u0026hellip;.\nContent Firewall "
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/5.4-tfc/",
	"title": "Terraform Cloud",
	"tags": [],
	"description": "",
	"content": "Introduction Terraform is a DevOps tool for declarative infrastructure—infrastructure as code. It simplifies and accelerates the configuration of cloud-based environments\nSource: Learn Github Repo First Setup Fork Code repo: Repository name: learning-terraform-3087701 Copy the main branch only AWS IAM user: tf-cloud Attach policies: AdministratorAccess Security credentials: Access key (Third-party service) Terraform Cloud Website: https://app.terraform.io/app Organizations: tuanta_learn Choose your workflow: Version Control Workflow Connect to a version control provider: Github.com Authorize cloud Choose a repository: learning-terraform-3087701 Workspace Name: learning-terraform Continue workspace overview -\u0026gt; Configure Variables Category: Environment variable Key : AWS_ACCESS_KEY_ID Value (sensitve) AWS_SECRET_ACCESS_KEY Value (sentive) Terraform Action Terraform plan instance_type = \u0026ldquo;t2.micro\u0026rdquo; region = \u0026ldquo;ap-southeast-1\u0026rdquo; provider.tf\rterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } variables.tf\r#variable \u0026#34;instance_type\u0026#34; { # description = \u0026#34;Type of EC2 instance to provision\u0026#34; # default = \u0026#34;t3.nano\u0026#34; #} outputs.tf\r#output \u0026#34;instance_ami\u0026#34; { # value = aws_instance.web.ami #} #output \u0026#34;instance_arn\u0026#34; { # value = aws_instance.web.arn #} main.tf\rdata \u0026#34;aws_ami\u0026#34; \u0026#34;app_ami\u0026#34; { most_recent = true filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;bitnami-tomcat-*-x86_64-hvm-ebs-nami\u0026#34;] } filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } owners = [\u0026#34;979382823631\u0026#34;] # Bitnami } resource \u0026#34;aws_instance\u0026#34; \u0026#34;web\u0026#34; { ami = data.aws_ami.app_ami.id instance_type = \u0026#34;t2.micro\u0026#34; tags = { Name = \u0026#34;HelloWorld\u0026#34; } } learning-terraform / Overview\nStart new plan Terraform run Confirm \u0026amp; Apply Fix Issue : No default VPC for this user\nConfigure file:\nAWS EC2 Key pair: tf-cloud-keypair\nprovider.tf\rterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-west-2\u0026#34; } terraform.tfvars\rvpc_cidr_block = \u0026#34;10.0.0.0/16\u0026#34; subnet_cidr_block = \u0026#34;10.0.10.0/24\u0026#34; avail_zone = \u0026#34;ap-southeast-1b\u0026#34; env_prefix = \u0026#34;dev\u0026#34; my_ip = \u0026#34;14.191.230.64/32\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; ami_id = \u0026#34;ami-06c4be2792f419b7b\u0026#34; main.tf\r# Variables variable vpc_cidr_block {} variable subnet_cidr_block {} variable avail_zone {} variable env_prefix {} variable instance_type {} variable ami_id {} # VPC \u0026amp; Subnet resource \u0026#34;aws_vpc\u0026#34; \u0026#34;myapp-vpc\u0026#34; { cidr_block = var.vpc_cidr_block tags = { Name = \u0026#34;${var.env_prefix}-vpc\u0026#34; } } resource \u0026#34;aws_subnet\u0026#34; \u0026#34;myapp-subnet-1\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id cidr_block = var.subnet_cidr_block availability_zone = var.avail_zone tags = { Name = \u0026#34;${var.env_prefix}-subnet-1\u0026#34; } } # Route table resource \u0026#34;aws_route_table\u0026#34; \u0026#34;myapp-route-table\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.myapp-igw.id } tags = { Name = \u0026#34;${var.env_prefix}-rtb\u0026#34; } } # Internet gateway resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;myapp-igw\u0026#34; { vpc_id = aws_vpc.myapp-vpc.id tags = { Name = \u0026#34;${var.env_prefix}-igw\u0026#34; } } # Associate subnet with Route Table resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;a-rtb-subnet\u0026#34; { subnet_id = aws_subnet.myapp-subnet-1.id route_table_id = aws_route_table.myapp-route-table.id } # Security Group variable my_ip {} resource \u0026#34;aws_security_group\u0026#34; \u0026#34;myapp-sg\u0026#34; { name = \u0026#34;myapp-sg\u0026#34; vpc_id = aws_vpc.myapp-vpc.id ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [var.my_ip] } ingress { from_port = 8080 to_port = 8080 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] prefix_list_ids = [] } tags = { Name = \u0026#34;${var.env_prefix}-sg\u0026#34; } } ### EC2 congifure resource \u0026#34;aws_instance\u0026#34; \u0026#34;myapp-server\u0026#34; { ami = var.ami_id instance_type = var.instance_type key_name = \u0026#34;tf-cloud-keypair\u0026#34; subnet_id = aws_subnet.myapp-subnet-1.id vpc_security_group_ids = [aws_security_group.myapp-sg.id] availability_zone = var.avail_zone associate_public_ip_address = true tags = { Name = \u0026#34;${var.env_prefix}-server\u0026#34; } } Run Terraform plan Run Terrafrom apply\nTerraform Cloud Overview\nAWS Console Check\nRun Terrafrom destroy\nSettings - Destruction and Deletion Destroy infrastructure -\u0026gt; Queue destroy plan "
},
{
	"uri": "http://localhost:1313/thedevops/4-cleanup/4.2-jenkins/",
	"title": "CICD",
	"tags": [],
	"description": "",
	"content": "We are processing \u0026hellip;.\n"
},
{
	"uri": "http://localhost:1313/thedevops/5-learn/",
	"title": "Learn",
	"tags": [],
	"description": "",
	"content": "Processing \u0026hellip;\nContent SRE IaC Github "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.5-linux/",
	"title": "Linux",
	"tags": [],
	"description": "",
	"content": "Project:\nFrontend: Vuejs Backend: Shoeshop Source:\nshoeshop-ecommerce.zip\rtodolist.zip\rvision.zip vi /etc/hosts\nmachine1\tlab-server\t192.168.33.100 lab.local.test\rmachine2\tgitlab-server\t192.168.33.110 gitlab.local.test\rmachine3\tmonitor-server\t192.168.33.120 monitor.local.test User \u0026amp; Working folder:\n/projects adduser todolist\rchown -R todolist:todolist /projects/todolist\rchmod -R 750 /projects/todolist Project process :\n1- Tools 2- Configuration files 3- Steps: build \u0026amp; run .\nSecurity: separate folder and user for each project\nContent Frontend Backend "
},
{
	"uri": "http://localhost:1313/thedevops/2-prepair/2.5-local/",
	"title": "Local",
	"tags": [],
	"description": "",
	"content": "Overview We will use local environment with Vagrant and VirtualBox to test best practices.\nVagrant Vagrant is an open-source tool for building and managing virtualized development environments. It helps developers create and configure reproducible and portable development environments that closely mimic production setups. Virtualbox VirtualBox is a powerful open-source virtualization software developed by Oracle Corporation. It allows users to run multiple guest operating systems (OS) simultaneously on a single physical machine. Configuration Requirement\nMachine 1: Linux-server IP: 192.168.33.100 Memory: 2048 Mb Machine 2: Jenkins-server IP: 192.168.33.110 Memory: 5120 Mb Machine 3: Monitor-server IP: 192.168.33.120 Memory: 2048 Mb Vagrantfile Expand:\r# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.box = \u0026#34;ubuntu/focal64\u0026#34; # Configuration for the first virtual machine config.vm.define \u0026#34;machine1\u0026#34; do |machine1| machine1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.100\u0026#34; machine1.vm.hostname = \u0026#34;linux-server\u0026#34; machine1.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end machine1.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; end # Configuration for the second virtual machine config.vm.define \u0026#34;machine2\u0026#34; do |machine2| machine2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.110\u0026#34; machine2.vm.hostname = \u0026#34;jenkins-server\u0026#34; machine2.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;5120\u0026#34; end machine2.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; end # Configuration for the third virtual machine config.vm.define \u0026#34;machine3\u0026#34; do |machine3| machine3.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.33.120\u0026#34; machine3.vm.hostname = \u0026#34;monitor-server\u0026#34; machine3.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = \u0026#34;2048\u0026#34; end machine3.vm.synced_folder \u0026#34;./datas\u0026#34;, \u0026#34;/vagrant_data\u0026#34; machine3.vm.provision \u0026#34;shell\u0026#34;, inline: \u0026lt;\u0026lt;-SHELL apt-get update SHELL end end Run\nvagrant up vagrant status vagrant ssh vagrant halt\rvagrant reload\rfor upgrade Ram, CPU\nvagrant destroy -f\rDestroy all machine\nWindows WSL \u0026amp; Vagrant Source review\nRequirements:\nWindows 10 Virtualbox WSL 2 Vagrant Vagrant plugin: vitualbox_WSL2 Install VirtualBox\nInstall WSL2: wsl -l -v\nInstall Powershell Preview\nInvoke-Expression \u0026#34;\u0026amp; { $(Invoke-Restmethod https://aka.ms/install-powershell.ps1) } -UseMSI -Preview\u0026#34; Install Vagrant (PS) # run inside WSL 2 # check https://www.vagrantup.com/downloads for more info curl -fsSL https://apt.releases.hashicorp.com/gpg | sudo apt-key add - sudo apt-add-repository \u0026#34;deb [arch=amd64] https://apt.releases.hashicorp.com $(lsb_release -cs) main\u0026#34; sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install vagrant Update Vagrant (PS): vagrant --version choco install vagrant --version 2.4.1 Enable WSL 2 support (The Terminal on WSL2) # append those two lines into ~/.bashrc echo \u0026#39;export VAGRANT_WSL_ENABLE_WINDOWS_ACCESS=\u0026#34;1\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;export PATH=\u0026#34;$PATH:/mnt/c/Program Files/Oracle/VirtualBox\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc # now reload the ~/.bashrc file source ~/.bashrc Install virtualbox_WSL2 plugin (The Terminal on WSL2) # Install virtualbox_WSL2 plugin vagrant plugin install virtualbox_WSL2 Install and configured (The Terminal on WSL2) # Go to Windows user\u0026#39;s dir from WSL cd /mnt/c/Users/\u0026lt;my-user-name\u0026gt;/ # Create a project dir mkdir -p projects/vagrant-demo cd projects/vagrant-demo # Create a Vagrantfile using Vagrant CLI vagrant init hashicorp/bionic64 ls -l Vagrantfile # Start a VM using Vagrantfile vagrant up # Login to the VM # (password is \u0026#39;vagrant\u0026#39;) vagrant ssh # Done :) Processing\rVagrantfile Vagrantfile (WSL): # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| # Machine 1 configuration config.vm.define \u0026#34;machine1\u0026#34; do |machine1| machine1.vm.box = \u0026#34;hashicorp/bionic64\u0026#34; machine1.vm.hostname = \u0026#34;machine1\u0026#34; machine1.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.101\u0026#34; machine1.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = 1024 vb.cpus = 1 end end # Machine 2 configuration config.vm.define \u0026#34;machine2\u0026#34; do |machine2| machine2.vm.box = \u0026#34;hashicorp/bionic64\u0026#34; machine2.vm.hostname = \u0026#34;machine2\u0026#34; machine2.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.56.102\u0026#34; machine2.vm.provider \u0026#34;virtualbox\u0026#34; do |vb| vb.memory = 1024 vb.cpus = 1 end end end vagrant ssh machine1 password: vagrant "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/6.6-docker/",
	"title": "Docker",
	"tags": [],
	"description": "",
	"content": "Install Project folder for tools:\nmkdir /tools cd /tools/ mkdir docker cd docker vi install-docker.sh chmod +x install-docker.sh ./install-docker.sh #!/bin/bash sudo apt update sudo apt install -y apt-transport-https ca-certificates curl software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg echo \u0026#34;deb [signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null sudo apt update sudo apt install -y docker-ce sudo systemctl start docker sudo systemctl enable docker sudo curl -L \u0026#34;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose docker --version docker-compose --version Run APP Ubuntu\ndocker pull ubuntu:22.04 docker run \u0026ndash;name ubuntu -it ubuntu:22.04 docker ps -a docker start 6e7 docker exec -it ubuntu bash Nginx\ndocker run \u0026ndash;name nginx -p 9999:80 nginx Check: http://192.168.33.100:9999 Clean docker rm -f $(docker ps -aq) Dockerfiles Overview: Dockerfiles để làm gì:\nĐưa source code vào container Cài đặt công cụ để chạy dự án Dockerfile command:\nFROM WORKDIR COPY RUN ENV EXPOSE 80 CMD ENTRYPOINT Tư duy viết Dockerfile tối ưu:\nNon root user Base image Multipe stage Content Workflow Advanced "
},
{
	"uri": "http://localhost:1313/thedevops/6-labs/",
	"title": "Labs",
	"tags": [],
	"description": "",
	"content": "Devops projects are processing \u0026hellip;..\nContent Jenkins GitLab Monitoring Security Linux Docker "
},
{
	"uri": "http://localhost:1313/thedevops/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/thedevops/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]